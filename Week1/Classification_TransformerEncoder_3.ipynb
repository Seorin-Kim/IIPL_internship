{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from tqdm import tqdm, tqdm_notebook, trange\n",
    "import sentencepiece as spm\n",
    "import wget\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data, datasets\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1e9a4bf4110>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 5\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "lr = 0.001\n",
    "EPOCHS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(sequential=True, batch_first=True, lower=True, fix_length=512)\n",
    "LABEL = data.Field(sequential=False, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = datasets.IMDB.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset 구성요소 출력 : {'text': <torchtext.data.field.Field object at 0x000001E990CA3DF0>, 'label': <torchtext.data.field.Field object at 0x000001E990D86490>}\n",
      "testset 구성요소 출력 : {'text': <torchtext.data.field.Field object at 0x000001E990CA3DF0>, 'label': <torchtext.data.field.Field object at 0x000001E990D86490>}\n"
     ]
    }
   ],
   "source": [
    "print('trainset 구성요소 출력 :', trainset.fields)\n",
    "print(\"testset 구성요소 출력 :\", testset.fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bromwell', 'high', 'is', 'a', 'cartoon', 'comedy.', 'it', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life,', 'such', 'as', '\"teachers\".', 'my', '35', 'years', 'in', 'the', 'teaching', 'profession', 'lead', 'me', 'to', 'believe', 'that', 'bromwell', \"high's\", 'satire', 'is', 'much', 'closer', 'to', 'reality', 'than', 'is', '\"teachers\".', 'the', 'scramble', 'to', 'survive', 'financially,', 'the', 'insightful', 'students', 'who', 'can', 'see', 'right', 'through', 'their', 'pathetic', \"teachers'\", 'pomp,', 'the', 'pettiness', 'of', 'the', 'whole', 'situation,', 'all', 'remind', 'me', 'of', 'the', 'schools', 'i', 'knew', 'and', 'their', 'students.', 'when', 'i', 'saw', 'the', 'episode', 'in', 'which', 'a', 'student', 'repeatedly', 'tried', 'to', 'burn', 'down', 'the', 'school,', 'i', 'immediately', 'recalled', '.........', 'at', '..........', 'high.', 'a', 'classic', 'line:', 'inspector:', \"i'm\", 'here', 'to', 'sack', 'one', 'of', 'your', 'teachers.', 'student:', 'welcome', 'to', 'bromwell', 'high.', 'i', 'expect', 'that', 'many', 'adults', 'of', 'my', 'age', 'think', 'that', 'bromwell', 'high', 'is', 'far', 'fetched.', 'what', 'a', 'pity', 'that', 'it', \"isn't!\"]\n",
      "pos\n"
     ]
    }
   ],
   "source": [
    "print(vars(trainset[0])['text'])\n",
    "print(vars(trainset[0])['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(trainset, min_freq=5)\n",
    "LABEL.build_vocab(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 46159\n",
      "클래스의 개수 : 2\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(TEXT.vocab)\n",
    "n_classes = 2\n",
    "print('단어 집합의 크기 : {}'.format(vocab_size))\n",
    "print('클래스의 개수 : {}'.format(n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, valset = trainset.split(split_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE = 64\n",
    "# [64, 512]로 고정되도록 패딩\n",
    "train_iter, val_iter, test_iter = data.BucketIterator.splits((trainset, valset, testset), batch_size=BATCH_SIZE, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 미니 배치의 개수 : 313\n",
      "테스트 데이터의 미니 배치의 개수 : 391\n",
      "검증 데이터의 미니 배치의 개수 : 79\n"
     ]
    }
   ],
   "source": [
    "print('훈련 데이터의 미니 배치의 개수 : {}'.format(len(train_iter)))\n",
    "print('테스트 데이터의 미니 배치의 개수 : {}'.format(len(test_iter)))\n",
    "print('검증 데이터의 미니 배치의 개수 : {}'.format(len(val_iter)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 512])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_iter))\n",
    "print(batch.text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 512])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_iter))\n",
    "print(batch.text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, val_iter, test_iter = data.BucketIterator.splits((trainset,valset,testset), batch_size=BATCH_SIZE, shuffle=True, repeat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "* reference    \n",
    "http://nlp.seas.harvard.edu/2018/04/03/attention.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"Define standard linear + softmax generation step.\"\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        drop = sublayer(self.norm(x))\n",
    "        return x + self.dropout(drop[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, n_heads, hid_dim, device, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert hid_dim % n_heads == 0\n",
    "        # We assume d_v always equals head_dim\n",
    "        self.head_dim = hid_dim // n_heads\n",
    "        self.n_heads = n_heads\n",
    "        self.hid_dim = hid_dim\n",
    "\n",
    "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
    "\n",
    "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \n",
    "        batch_size = query.shape[0]\n",
    "\n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "\n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0,2,1,3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0,2,1,3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0,2,1,3)\n",
    "\n",
    "        energy = torch.matmul(Q, K.permute(0,1,3,2)) / self.scale\n",
    "\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask==0, -1e10)\n",
    "        attention = torch.softmax(energy, dim=-1)\n",
    "\n",
    "        x = torch.matmul(self.dropout(attention),V)\n",
    "        x = x.permute(0,2,1,3).contiguous()\n",
    "        x = x.view(batch_size, -1, self.hid_dim)\n",
    "        x = self.fc_o(x)\n",
    "\n",
    "        return x, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], \n",
    "                         requires_grad=False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "        \n",
    "    def __init__(self, n_token, n_dim_model, n_head, n_hidden, n_blocks, src_pad_idx, device, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        # #Multi Headed Attention Layer\n",
    "        self_attention = MultiHeadedAttention(n_head, n_dim_model, device)\n",
    "        # #Feedforward Layer\n",
    "        feed_forward = PositionwiseFeedForward(n_dim_model, n_hidden, dropout)\n",
    "        # #Positional Encoding\n",
    "        positional_encoding = PositionalEncoding(n_dim_model, dropout)\n",
    "        \n",
    "        encoder_layer = EncoderLayer(n_dim_model, copy.deepcopy(self_attention), copy.deepcopy(feed_forward), dropout)\n",
    "        self.encoder = Encoder(encoder_layer, n_blocks)\n",
    "\n",
    "        embedding = Embeddings(n_dim_model, n_token)\n",
    "        self.src_embed = nn.Sequential(embedding, copy.deepcopy(positional_encoding))\n",
    "        \n",
    "        # Fully-Connected Layer\n",
    "        self.fc = nn.Linear(n_dim_model, 2)\n",
    "\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "\n",
    "\n",
    "    # 소스 문장의 <pad> 토큰에 대하여 마스크(mask) 값을 0으로 설정\n",
    "    def make_src_mask(self, src):\n",
    "\n",
    "        # src: [batch_size, src_len]\n",
    "\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # src_mask: [batch_size, 1, 1, src_len]\n",
    "\n",
    "        return src_mask\n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        mask = self.make_src_mask(x)\n",
    "\n",
    "        # # x dimension[k, batch_size = 64]\n",
    "        embedded_sents = self.src_embed(x)\n",
    "        encoded_sents = self.encoder(embedded_sents, mask)\n",
    "        \n",
    "       \n",
    "        final_feature_map = encoded_sents[:,-1,:] \n",
    "        final_out = self.fc(final_feature_map) \n",
    "        \n",
    "        return final_out "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "MODEL_DIM = 512\n",
    "HIDDEN_DIM = 256\n",
    "N_LAYERS = 3\n",
    "N_HEADS = 8\n",
    "DROPOUT_RATIO = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "LABEL_PAD_IDX = LABEL.vocab.stoi[LABEL.pad_token]\n",
    "\n",
    "model = TransformerModel(INPUT_DIM, MODEL_DIM, N_HEADS, HIDDEN_DIM, N_LAYERS, TEXT_PAD_IDX, DEVICE, DROPOUT_RATIO)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 27,582,210 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (encoder): Encoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (w_2): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (w_2): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (w_2): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm()\n",
       "  )\n",
       "  (src_embed): Sequential(\n",
       "    (0): Embeddings(\n",
       "      (lut): Embedding(46159, 512)\n",
       "    )\n",
       "    (1): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "\n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, train_iter, device):\n",
    "    model.train()\n",
    "    for b, batch in enumerate(train_iter):\n",
    "        x, y = batch.text.to(device), batch.label.to(device)\n",
    "        y.data.sub_(1)  # 레이블 값을 0과 1로 변환\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logit = model(x)\n",
    "        loss = criterion(logit, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, criterion, val_iter, device):\n",
    "    \"\"\"evaluate model\"\"\"\n",
    "    model.eval()\n",
    "    corrects, total_loss = 0, 0\n",
    "    for batch in val_iter:\n",
    "        x, y = batch.text.to(device), batch.label.to(device)\n",
    "        y.data.sub_(1) # 레이블 값을 0과 1로 변환\n",
    "        logit = model(x)\n",
    "        loss = criterion(logit, y)\n",
    "        total_loss += loss.item()\n",
    "        corrects += (logit.max(1)[1].view(y.size()).data == y.data).sum()\n",
    "    size = len(val_iter.dataset)\n",
    "    avg_loss = total_loss / size\n",
    "    avg_accuracy = 100.0 * corrects / size\n",
    "    return avg_loss, avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 1] val loss :  0.01 | val accuracy : 80.54\n",
      "[Epoch: 2] val loss :  0.01 | val accuracy : 78.00\n",
      "[Epoch: 3] val loss :  0.01 | val accuracy : 73.98\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\IIPL_internship\\Week1\\Classification_TransformerEncoder_2.ipynb 셀 39\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/IIPL_internship/Week1/Classification_TransformerEncoder_2.ipynb#X53sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m EPOCH \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/IIPL_internship/Week1/Classification_TransformerEncoder_2.ipynb#X53sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, EPOCHS\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/IIPL_internship/Week1/Classification_TransformerEncoder_2.ipynb#X53sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     train(model, optimizer, criterion, train_iter)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/IIPL_internship/Week1/Classification_TransformerEncoder_2.ipynb#X53sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     val_loss, val_accuracy \u001b[39m=\u001b[39m evaluate(model, criterion, val_iter)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/IIPL_internship/Week1/Classification_TransformerEncoder_2.ipynb#X53sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m[Epoch: \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m] val loss : \u001b[39m\u001b[39m%5.2f\u001b[39;00m\u001b[39m | val accuracy : \u001b[39m\u001b[39m%5.2f\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (e, val_loss, val_accuracy))\n",
      "\u001b[1;32md:\\IIPL_internship\\Week1\\Classification_TransformerEncoder_2.ipynb 셀 39\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, criterion, train_iter)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/IIPL_internship/Week1/Classification_TransformerEncoder_2.ipynb#X53sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m logit \u001b[39m=\u001b[39m model(x)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/IIPL_internship/Week1/Classification_TransformerEncoder_2.ipynb#X53sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(logit, y)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/IIPL_internship/Week1/Classification_TransformerEncoder_2.ipynb#X53sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/IIPL_internship/Week1/Classification_TransformerEncoder_2.ipynb#X53sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\asus\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\asus\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_val_loss = None\n",
    "for e in range(1, EPOCHS+1):\n",
    "    train(model, optimizer, criterion, train_iter, DEVICE)\n",
    "    val_loss, val_accuracy = evaluate(model, criterion, val_iter, DEVICE)\n",
    "\n",
    "    print(\"[Epoch: %d] val loss : %5.2f | val accuracy : %5.2f\" % (e, val_loss, val_accuracy))\n",
    "\n",
    "    \"\"\"\n",
    "    # 검증 오차가 가장 적은 최적의 모델을 저장\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        if not os.path.isdir(\"snapshot\"):\n",
    "            os.makedirs(\"snapshot\")\n",
    "        torch.save(model.state_dict(), 'D:/IIPL_internship/Week1/txtclassification.pt')\n",
    "        best_val_loss = val_loss\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\IIPL_internship\\Week1\\Classification_TransformerEncoder_2.ipynb 셀 40\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/IIPL_internship/Week1/Classification_TransformerEncoder_2.ipynb#X54sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m test_loss, test_accuracy \u001b[39m=\u001b[39m evaluate(model, criterion, test_iter)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/IIPL_internship/Week1/Classification_TransformerEncoder_2.ipynb#X54sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mtest loss : \u001b[39m\u001b[39m%5.2f\u001b[39;00m\u001b[39m | test accuracy : \u001b[39m\u001b[39m%5.2f\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (test_loss, test_accuracy))\n",
      "\u001b[1;32md:\\IIPL_internship\\Week1\\Classification_TransformerEncoder_2.ipynb 셀 40\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(model, criterion, val_iter)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/IIPL_internship/Week1/Classification_TransformerEncoder_2.ipynb#X54sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m x, y \u001b[39m=\u001b[39m batch\u001b[39m.\u001b[39mtext\u001b[39m.\u001b[39mto(DEVICE), batch\u001b[39m.\u001b[39mlabel\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/IIPL_internship/Week1/Classification_TransformerEncoder_2.ipynb#X54sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m y\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39msub_(\u001b[39m1\u001b[39m) \u001b[39m# 레이블 값을 0과 1로 변환\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/IIPL_internship/Week1/Classification_TransformerEncoder_2.ipynb#X54sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m logit \u001b[39m=\u001b[39m model(x)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/IIPL_internship/Week1/Classification_TransformerEncoder_2.ipynb#X54sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(logit, y)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/IIPL_internship/Week1/Classification_TransformerEncoder_2.ipynb#X54sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\asus\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32md:\\IIPL_internship\\Week1\\Classification_TransformerEncoder_2.ipynb 셀 40\u001b[0m in \u001b[0;36mTransformerModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/IIPL_internship/Week1/Classification_TransformerEncoder_2.ipynb#X54sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m# # x dimension[k, batch_size = 64]\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/IIPL_internship/Week1/Classification_TransformerEncoder_2.ipynb#X54sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m embedded_sents \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msrc_embed(x)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/IIPL_internship/Week1/Classification_TransformerEncoder_2.ipynb#X54sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m encoded_sents \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(embedded_sents, mask)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/IIPL_internship/Week1/Classification_TransformerEncoder_2.ipynb#X54sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m final_feature_map \u001b[39m=\u001b[39m encoded_sents[:,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,:] \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/IIPL_internship/Week1/Classification_TransformerEncoder_2.ipynb#X54sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m final_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(final_feature_map) \n",
      "File \u001b[1;32mc:\\Users\\asus\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32md:\\IIPL_internship\\Week1\\Classification_TransformerEncoder_2.ipynb 셀 40\u001b[0m in \u001b[0;36mEncoder.forward\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/IIPL_internship/Week1/Classification_TransformerEncoder_2.ipynb#X54sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m\"\u001b[39m\u001b[39mPass the input (and mask) through each layer in turn.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/IIPL_internship/Week1/Classification_TransformerEncoder_2.ipynb#X54sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/IIPL_internship/Week1/Classification_TransformerEncoder_2.ipynb#X54sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     x \u001b[39m=\u001b[39m layer(x, mask)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/IIPL_internship/Week1/Classification_TransformerEncoder_2.ipynb#X54sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(x)\n",
      "File \u001b[1;32mc:\\Users\\asus\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32md:\\IIPL_internship\\Week1\\Classification_TransformerEncoder_2.ipynb 셀 40\u001b[0m in \u001b[0;36mEncoderLayer.forward\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/IIPL_internship/Week1/Classification_TransformerEncoder_2.ipynb#X54sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, mask):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/IIPL_internship/Week1/Classification_TransformerEncoder_2.ipynb#X54sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mFollow Figure 1 (left) for connections.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/IIPL_internship/Week1/Classification_TransformerEncoder_2.ipynb#X54sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msublayer[\u001b[39m0\u001b[39;49m](x, \u001b[39mlambda\u001b[39;49;00m x: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(x, x, x, mask))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/IIPL_internship/Week1/Classification_TransformerEncoder_2.ipynb#X54sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msublayer[\u001b[39m1\u001b[39m](x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeed_forward)\n",
      "File \u001b[1;32mc:\\Users\\asus\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32md:\\IIPL_internship\\Week1\\Classification_TransformerEncoder_2.ipynb 셀 40\u001b[0m in \u001b[0;36mSublayerConnection.forward\u001b[1;34m(self, x, sublayer)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/IIPL_internship/Week1/Classification_TransformerEncoder_2.ipynb#X54sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, sublayer):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/IIPL_internship/Week1/Classification_TransformerEncoder_2.ipynb#X54sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mApply residual connection to any sublayer with the same size.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/IIPL_internship/Week1/Classification_TransformerEncoder_2.ipynb#X54sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     drop \u001b[39m=\u001b[39m sublayer(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm(x))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/IIPL_internship/Week1/Classification_TransformerEncoder_2.ipynb#X54sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(drop[\u001b[39m0\u001b[39m])\n",
      "\u001b[1;32md:\\IIPL_internship\\Week1\\Classification_TransformerEncoder_2.ipynb 셀 40\u001b[0m in \u001b[0;36mEncoderLayer.forward.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/IIPL_internship/Week1/Classification_TransformerEncoder_2.ipynb#X54sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, mask):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/IIPL_internship/Week1/Classification_TransformerEncoder_2.ipynb#X54sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mFollow Figure 1 (left) for connections.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/IIPL_internship/Week1/Classification_TransformerEncoder_2.ipynb#X54sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msublayer[\u001b[39m0\u001b[39m](x, \u001b[39mlambda\u001b[39;00m x: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(x, x, x, mask))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/IIPL_internship/Week1/Classification_TransformerEncoder_2.ipynb#X54sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msublayer[\u001b[39m1\u001b[39m](x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeed_forward)\n",
      "File \u001b[1;32mc:\\Users\\asus\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32md:\\IIPL_internship\\Week1\\Classification_TransformerEncoder_2.ipynb 셀 40\u001b[0m in \u001b[0;36mMultiHeadedAttention.forward\u001b[1;34m(self, query, key, value, mask)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/IIPL_internship/Week1/Classification_TransformerEncoder_2.ipynb#X54sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mif\u001b[39;00m mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/IIPL_internship/Week1/Classification_TransformerEncoder_2.ipynb#X54sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     energy \u001b[39m=\u001b[39m energy\u001b[39m.\u001b[39mmasked_fill(mask\u001b[39m==\u001b[39m\u001b[39m0\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1e10\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/IIPL_internship/Week1/Classification_TransformerEncoder_2.ipynb#X54sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m attention \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49msoftmax(energy, dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/IIPL_internship/Week1/Classification_TransformerEncoder_2.ipynb#X54sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(attention),V)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/IIPL_internship/Week1/Classification_TransformerEncoder_2.ipynb#X54sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m3\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = evaluate(model, criterion, test_iter, DEVICE)\n",
    "print(\"test loss : %5.2f | test accuracy : %5.2f\" % (test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 32-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "00353a74eb640e76c3ea96069a586a96e04a8f85b363aa6df5eeebeb90fb1f4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
